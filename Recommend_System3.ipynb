{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommend System3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKf/zgGSfixNdquVzEVFha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimuksung/kakao_Recommend/blob/master/Recommend_System3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmbg8qW4k4xV",
        "colab_type": "text"
      },
      "source": [
        "# Candidate generation\n",
        "1. Content based\n",
        "- Item 간의 유사도를 추출\n",
        "- Item의 Feature\n",
        "- 사용자의 이전 행동(click, 시청, 구매 내역) + 명시적 피드백(평점 좋아요)를 기반으로 사용자가 좋아하는 것을 추천한다.\n",
        "\n",
        "Ex) user A 가 cat을 봤다면 다른 animal 추천\n",
        "\n",
        "<img src=\"https://k.kakaocdn.net/dn/c4bf7O/btqCjSyt8Oe/ypWuLgLQluTV9IAHKYKrb1/img.png\"/>\n",
        "\n",
        "- User는 Education Science healthcare를 봤다고 가정하면 각각의 feature들은 0이거나 조금이라도 값을 가지고 있기 때문에 Dot product를 이용하여 항목에 점수를 매기는 가장 좋다.\n",
        "- Dot Product를 사용한다면 사용자 embedding x와 app embedding y가 이진 vector로 나타내진다.\n",
        "- x,y를 전부 더하면 1이 된다. 다시 말해 <x,y>는 동시에 활성화 되는 feature의 수이다. 높은 내적은 공통적인 특징을 나타내므로 높은 유사성을 띈다.\n",
        "- 장점 : Model은 다른 사용자의 Data가 필요하지 않고 App의 Data만 필요하다.\n",
        "- 단점 : Feature를 표현하기 위해 수작업으로 설계하는 과정이 필요하다. + 도메인 지식이 많이 필요하기 때문에 Feature가 어떻게 설계되었느냐에 따라 크게 달라질 수 있다.\n",
        "\n",
        "\n",
        "2. Collaborative filtering\n",
        "- Query + Item 유사도\n",
        "Ex) user A와 user B가 유사하다면 user A가 좋아하는 것 중 하나를 B에게 추천\n",
        "\n",
        "- User 와 영화를 나타내려고 한다.\n",
        "- 영화 -> 1차원 embedding\n",
        "<img src=\"https://k.kakaocdn.net/dn/bjrUHB/btqCjRM5jx0/3xvQeLKf2c3uiKjqohTTqK/img.png\" />\n",
        "<br/>\n",
        "\n",
        "- 2차원 embedding\n",
        "\n",
        "<img src=\"https://k.kakaocdn.net/dn/nPb5e/btqCjSrEJJO/8p4NSj5SFQKqbLZlSREwgK/img.png\"/>\n",
        "\n",
        "<img src=\"https://k.kakaocdn.net/dn/dmqXET/btqChHdj9Jp/2SD909dNMPHy36oUKwPdD1/img.png\"/>\n",
        "\n",
        "- Matrix Factorization\n",
        "user item 2차원 embedding을 하게 되어 아래와 같은 형태로 나타내지는데 4x2 2x5 matrix를 이용하여 4x5를 유추(SVM 알고리즘)에 의하여 vector의 크기만 바뀔 뿐 방향을 달라지지 않는다. 이를 이용하여 cos 유사도로 추천 시스템을 만드는 것.\n",
        "\n",
        "<img src=\"https://k.kakaocdn.net/dn/JrVIT/btqClxHjE7L/OKNvuaVNdyT20KEnhlQpgK/img.png\" />\n",
        "\n",
        "# object function\n",
        "\n",
        "- UV^T 의 내적(Dot Product)이 피드백 매트릭스 A의 근삿값이 되도록 임베딩을 학습\n",
        "\n",
        "- Matrix A가 sparse(희박)하다면 SVD는 훌륭한 solution이 될 수 없다. \n",
        "Ex) 만약 Youtube의 게임 category만 보는 사람만 모으면, 일반화 성능이 저하된다.\n",
        "\n",
        "- 이를 방지하기 위해 관찰된 항목에 대한 합계와, 관찰되지 않은 항목에 대한 합계(0으로 처리)이다.\n",
        "min\n",
        "U\n",
        "∈\n",
        "R\n",
        "m\n",
        "×\n",
        "d\n",
        ",\n",
        "V\n",
        "∈\n",
        "R\n",
        "n\n",
        "×\n",
        "d\n",
        "∑\n",
        "(\n",
        "i\n",
        ",\n",
        "j\n",
        ")\n",
        "∈\n",
        "o\n",
        "b\n",
        "s\n",
        "(\n",
        "A\n",
        "i\n",
        "j\n",
        "−\n",
        "⟨\n",
        "U\n",
        "i\n",
        ",\n",
        "V\n",
        "j\n",
        "⟩\n",
        ")\n",
        "2\n",
        "+\n",
        "w\n",
        "0\n",
        "∑\n",
        "(\n",
        "i\n",
        ",\n",
        "j\n",
        ")\n",
        "∉\n",
        "o\n",
        "b\n",
        "s\n",
        "(\n",
        "⟨\n",
        "U\n",
        "i\n",
        ",\n",
        "V\n",
        "j\n",
        "⟩\n",
        ")\n",
        "2\n",
        "\n",
        "# object function generation\n",
        "\n",
        "1) SGD(확률적 경사 하강법)\n",
        "- 매우 유연하여 다른 Loss function을 사용할 수 있다.\n",
        "\n",
        "- 병렬화 할 수 있다.\n",
        "\n",
        "- 느리다 + 빠르게 수렴하지 않는다.\n",
        "\n",
        "- 관찰되지 않은 항목을 다루기가 더 어렵다(Negative Sampling 또는 Gravity를 사용해야 함)\n",
        "\n",
        "2) WALS(Weighted Alternating Least Squares)\n",
        "- 병렬화 할 수 있다.\n",
        "\n",
        "- SGD보다 빠르게 수렴한다.\n",
        "\n",
        "- 관찰되지 않은 항목을 보다 쉽게 처리할 수 있다.\n",
        "\n",
        "- Loss Squares에만 의존한다.\n",
        "\n",
        "\n",
        "\n",
        "# Embedding\n",
        "- 차원 축소 후 데이터가 embedding\n",
        "- embedding space는 일반적으로 차원이 낮으며, 유사한 data는 가깝게 위치 시킨다.\n",
        "\n",
        "# 유사도 측정\n",
        "- 여러 vector를 가져와 유사성을 측정하는 스칼라를 반환\n",
        "- Cos / Dot product / Euclidean distance\n",
        "\n",
        "<img src=\"https://k.kakaocdn.net/dn/Nzgma/btqCoQGE0k6/1AS79xf9KCtRN5E9Kuoga0/img.jpg\" />\n",
        "\n",
        "<img src=\"https://k.kakaocdn.net/dn/nx0Ds/btqCn0bL8MM/0MOa012raDZPTLP3O7HP8K/img.jpg\" />\n",
        "\n",
        "- Dot product는 Norm에 민감하기 때문에 Youtube의 인기 동영상과 같은 인기도를 추천하는데 도움이 된다.\n",
        "하지만! 주의해서 사용하지 않는다면 인기있는 Item만 추천하지는 않는다."
      ]
    }
  ]
}